<!doctype html>

<html lang="en">
<head>
	<title>Hacker News Daily</title>
	<meta name="description" content="Lightweight daily best Hacker News posts, with screenshots and top comments. No JavaScript used.">
	<meta name="author" content="Pedro Lopes">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta charset="utf-8">
	<link rel="stylesheet" href="css/styles.css?v=1.0">
	<link href="/favicon.png" rel="icon">
</head>

<body>

<div class="container">

	<div class="navbar">
		<div class="navbar__timespan">
			
				<span class="navbar__timespan_item-current navbar__timespan_item-spacing">Latest</span>
			
			
				<a href='all_without_images.html'>All</a>
			
		</div>
		<div class="navbar__no-images">
			
				<a href='index.html'
				>Enable Previews</a>
			
		</div>
	</div>

	
	<div class="day__date">22 September 2022</div>
		
		<div class="story story_small-padding-bottom">
			
			<div >
				<div class="story__title">
					<h3><a href="https://openai.com/blog/whisper/">Whisper – open source speech recognition by OpenAI</a></h3>
				</div>
				<div class="story__comments">
					<div class="story__comment">
						<a href="https://news.ycombinator.com/item?id=32927360">
						
							
								<p class="story__comment_first line-clamp">"Neat, https://github.com/openai/whisper - they have open-sourced it, even the model weights, so they are living up to their name in this instance.The 4 examples are stunningly good (the examples have speakers with heavy accents, speaking in foreign language, speaking with dynamic background noise, etc.), this is far and away better than anything else I&#39;ve seen. Will be super curious to see other folks trying it out and seeing if it&#39;s as robust as it seems, including when confronted with audio speech with natural tics and uhhh&#39;s and uhmm&#39;s and everything in-between.I think it&#39;s fair to say that AI-transcription accuracy is now decidedly superior to the average human&#39;s, what the implications of this are I&#39;m not sure."</p>
							
						
							
								<p class="story__comment_others">"It seems like OpenAI are finally living up to their name for once with this release? Anything I&#39;m missing?From what I can gather:1. Includes model weights. I can&#39;t find the URL, but they reference them enough and have a CLI tool, so I presume I just haven&#39;t found them yet.2. Includes code: https://github.com/openai/whisper3. Released under MIT License: https://github.com/openai/whisper/blob/main/LICENSE"</p>
							
						
							
								<p class="story__comment_others">"Hey this looks great! I like to record audio notes while driving in my car after work, to kind of decompress my thoughts from the day. But I never go back and listen as they can be long and meandering. Sometimes in the audio log I will sum up my thoughts, but this might be 20 minutes in and hard to find. I really wish I had transcriptions so I could easily scan the full contents. I have tried Mozilla Deepspeech (I don&#39;t want a cloud solution) and I was surprised to find that I could not get Deepspeech to reliably transcribe them. There is a bit of road noise, though I think for a human listener they are easy to understand. It looks like this one might actually do the trick!EDIT: Tried it and it worked great! It is very easy to use. I just did the pip install line in the readme and was ready to go. You literally just run the one pip install line, and then you run the program in the format &#34;whisper my_audio.wav&#34; and it goes. Really nice job OpenAI!"</p>
							
						
						</a>
					</div>
				</div>
			</div>
		</div>
		
		<div class="story story_small-padding-bottom">
			
			<div >
				<div class="story__title">
					<h3><a href="https://justine.lol/cosmopolitan/functions.html">Can I use a system call?</a></h3>
				</div>
				<div class="story__comments">
					<div class="story__comment">
						<a href="https://news.ycombinator.com/item?id=32920227">
						
							
								<p class="story__comment_first line-clamp">"I used to work with someone who was an expert in writing cryptography libraries. He insisted on never using any system calls for maximum portability (he also wrote his own memory allocators and optimized for small binary size). Seemed quirky to me but he was proven right multiple times as our ultraportable library let us book some big deals that would have been otherwise infeasible without a big rewrite"</p>
							
						
							
								<p class="story__comment_others">"The coverage and the attention to detail here is incredible. Hats off to Justine.I would love to know how this table is generated and what the test process is. What&#39;s the best way to run tests like this across such a wide variety of OSes? Maybe vagrant images?"</p>
							
						
							
								<p class="story__comment_others">"I think the title would benefit from mentioning that this is for cosmopolitan libc. I was expecting this to be about syscall portability in general."</p>
							
						
						</a>
					</div>
				</div>
			</div>
		</div>
		
		<div class="story story_small-padding-bottom">
			
			<div >
				<div class="story__title">
					<h3><a href="https://fly.io/blog/introducing-litefs/">LiteFS</a></h3>
				</div>
				<div class="story__comments">
					<div class="story__comment">
						<a href="https://news.ycombinator.com/item?id=32925734">
						
							
								<p class="story__comment_first line-clamp">"The pain is that this approach is suitable on VPS/IaaS where disk volume is supported. As a solo dev, I only use PaaS kind of infra, there are just a few PaaS i&#39;m aware of that support attachable disk. Fly, Render, .. nothing else?"</p>
							
						
							
								<p class="story__comment_others">"This is distributed SQLite 3, running (I assume at least partially managed?) LiteFS[5] for you. Which is pretty cool!What I&#39;d like to have seen is how this compares to things like rqlite[1] or Cloudflare&#39;s D1[2] addressed directly in the articleThat said, I think this is pretty good for things like read replica&#39;s. I know the sales pitch here is as a full database, and I don&#39;t disagree with it, and if I was starting from scratch today and could use this, I totally would give it a try and benchmark / test accordingly, however I can&#39;t speak to that use case directly.What I find however and what I can speak to, is that most workloads already have database of some kind setup, typically not SQLite as their main database (MySQL or PostgreSQL seem most common). This is a great way to make very - insanely, really - fast read replica&#39;s across regions of your data. You can use an independent raft[3][4] implementation to do this on write. If your database supports it, you can even trigger a replication directly from a write to the database itself (I think Aurora has this ability, and I think - don&#39;t quote me! - PostgreSQL can do this natively via an extension to kick off a background job)To that point, in my experience one thing SQLite is actually really good at is storing JSON blobs. I have successfully used it for replicating JSON representations of read only data in the past to great success, cutting down on read times significantly for APIs as the data is &#34;pre-baked&#34; and the lightweight nature of SQLite allows you to - if you wanted to naively do this - just spawn a new database for each customer and transform their data accordingly ahead of time. Its like AOT compilation for your data.if you want to avoid some complexity with sharding (you can&#39;t always avoid it outright, but this can help cap its complexity) this approach helps enormously in my experience. Do try before you buy!EDIT: Looks like its running LiteFS[5] not LiteStream[0]. This is my error of understanding.[0]: https://litestream.io/[1]: https://github.com/rqlite/rqlite[2]: https://blog.cloudflare.com/introducing-d1/[3]: https://raft.github.io/[4]: https://raft.github.io/#implementations[5]: https://github.com/superfly/litefs"</p>
							
						
							
								<p class="story__comment_others">"&gt; Developing against a relational database requires devs to watch out for &#34;N+1&#34; query patterns, where a query leads to a loop that leads to more queries. N+1 queries against Postgres and MySQL can be lethal to performance. Not so much for SQLite.This is misleading AFAICT. The article(s) is actually comparing remote RDBMS to local RDBMS, not Postgres to SQLite.Postgres can also be served over a UNIX socket, removing the individual query overhead due to TCP roundtrip.SQLite is a great technology, but keep in mind that you can also deploy Postgres right next to your app as well. If your app is something like a company backend that could evolve a lot and benefit from Postgres&#39;s advanced features, this may be the right choice."</p>
							
						
						</a>
					</div>
				</div>
			</div>
		</div>
		
		<div class="story story_small-padding-bottom">
			
			<div >
				<div class="story__title">
					<h3><a href="https://frame.work/at/en/blog/introducing-the-framework-laptop-chromebook-edition">The Framework Laptop Chromebook Edition</a></h3>
				</div>
				<div class="story__comments">
					<div class="story__comment">
						<a href="https://news.ycombinator.com/item?id=32926369">
						
							
								<p class="story__comment_first line-clamp">"I hesitated posting this, because I don&#39;t want to be too negative, but: ugh.  ChromeOS is just more Google adware/tracking-ware, locking people into the Google ecosystem, and (by default, at least) creating a more locked-down environment than a general-purpose OS would have (not quite iOS or even Android, but still not with the flexibility of a &#34;mainstream&#34; OS).  I feel like Framework could be spending their time doing much better things.  Granted, if they believe that this will be a big boost to their bottom line / margins / sustainability, then I&#39;m in favor of it on the grounds of helping make sure Framework is a successful company.My hope was that this is just running on the standard Framework laptop hardware, but it looks like it required a bit of a mainboard redesign, as well as a different input cover and keyboard.  Extra hardware like that just makes their offering more difficult for a customer to navigate and understand, not to mention the added support and manufacturing burden on the company&#39;s side."</p>
							
						
							
								<p class="story__comment_others">"I&#39;m happy to answer questions anyone has on this product!"</p>
							
						
							
								<p class="story__comment_others">"I gave up waiting for the AMD version. Intel must be able to turn the screws down really tight on small shops. I wonder how it is done, exactly."</p>
							
						
						</a>
					</div>
				</div>
			</div>
		</div>
		
		<div class="story story_small-padding-bottom">
			
			<div >
				<div class="story__title">
					<h3><a href="https://www.nasa.gov/feature/goddard/2022/new-webb-image-captures-clearest-view-of-neptune-s-rings-in-decades/">New Webb image captures clearest view of Neptune’s rings in decades</a></h3>
				</div>
				<div class="story__comments">
					<div class="story__comment">
						<a href="https://news.ycombinator.com/item?id=32927156">
						
							
								<p class="story__comment_first line-clamp">"A little historical tangent:Tomorrow (23rd) marks the 176th anniversary  of the discovery of Neptune (~ 1.07 years on Neptune; 164.8 Earth years).Johann Gottfried Galle an astronomer at the Berlin observatory found Neptune at the behest of Urbain Le Verrier who calculated its position by hypothesizing that some body must perturb Uranus orbit to explain the distinct deviation from the predicted motion by Newton&#39;s law of universal gravitation.Le Verrier couldn&#39;t find a single French astronomer willing to take a look out at the sky after presenting his extensive calculations to the French Academy on 31 August. On 18 September he then wrote a letter to the German astronomer Galle at the Berlin observatory.After receiving the letter 5 days later Galle immediately set up his telescope the same night and found a +8 mag star (within 1 degree of the Le Verrier&#39;s calculation) not listed on the official Prussian star chart. The next night he managed to measure the star itself moving by 4 arcseconds, finally confirming Le Verrier prediction of a new planet (and to this day last).At ~30 AU (30x the distance sun-earth; Uranus being at ~20 AU) the discovery instantaneously made the solar system 1.5x bigger ;)"</p>
							
						
							
								<p class="story__comment_others">"Wow! If anyone is curious as to why Triton is so bright...&gt; Covered in a frozen sheen of condensed nitrogen, Triton reflects an average of 70 percent of the sunlight that hits it. It far outshines Neptune in this image because the planet’s atmosphere is darkened by methane absorption at these near-infrared wavelengths.That&#39;s pretty neat!"</p>
							
						
							
								<p class="story__comment_others">"I really hope to see a Cassini or Juno like probe to visit Neptune and Uranus in my lifetime.As good as JWST is and these images are, these objects are so distant the only high-res imagery you can get is by visiting them. Prior to New Horizons, our best image of Pluto, for example, was a few blurry pixels and that remained the case even until New Horizons was 99% of the way there."</p>
							
						
						</a>
					</div>
				</div>
			</div>
		</div>
		
		<div class="story story_small-padding-bottom">
			
			<div >
				<div class="story__title">
					<h3><a href="https://openwebsearch.eu/">EU Open Web Search project kicked off</a></h3>
				</div>
				<div class="story__comments">
					<div class="story__comment">
						<a href="https://news.ycombinator.com/item?id=32915263">
						
							
								<p class="story__comment_first line-clamp">"Interesting to see the amount of negative comments.Most of the negativity seems to come from the following points:- EU-funded project cannot succeed in tech because previous EU-funded projects have failed in tech in the past (and generally government-funded project in tech are suspicious)- Search is hard, and therefore it will fail- The project is underfundedEven if the points above can all be valid (though the obvious US-funded startup launched by a bunch of uni students seemed to have fared pretty well) it seems we are missing the point of this project.The proposal is to contribute to the creation of open building blocks necessary to enable others (including private US companies) to make better search products.Better search product are needed.Shall we remember HN of some of the intense conversations that happened here about Google failing us:- Google Search is Dying [1]- Every Google result now looks like an ad [2]- Google no longer producing high quality search results in significant categories [3]So while, yes search is a hard topic, we should welcome initiatives aiming at improving the ground infrastructure needed to lower the barrier to entry on this subject and hope it will allow many companies to build better search products (or inspire other initiatives to contribute in similar and even more successful way)1: https://news.ycombinator.com/item?id=303477192: https://news.ycombinator.com/item?id=221078233: https://news.ycombinator.com/item?id=29772136"</p>
							
						
							
								<p class="story__comment_others">"I have written this before but I’ll put it here again. What I would like to see is a federated search engine. Based on activitypub that works like mastodon. Don’t like the results from one source? Just remove them from your sources, or lower their ranking. Similar to yacy but you can work with the protocol to connect or build whatever type of index you want using whatever technology you like, and communicate over an existing standard. Want to build the worlds best index of Pokémon sites, then go do it. Want to build a search engine using idris or ats? Sure! I did note the professors are on mastodon so perhaps this may actually happen.One of these days I’ll actually implement the above assuming nobody else does. I figured if I can at least get the basics done and a reference implementation that’s easy to run it could prove the concept. If anyone is interested in this do email my in my bio.What I worry about for this project is that it becomes another island which prohibits remixing of results like google and bing, and its own index and ranking algorithms become gamed.I wish the creators best of luck though. I am also hoping for some more blogs and papers about the internals of he engine. So little information is published in the space that anything is welcome, especially if it’s deeply technical."</p>
							
						
							
								<p class="story__comment_others">"Seriously... I really wanted to like this project, but it seems everything EU touches as of recently gets worse.From the webpafe that half of the time shows &#34;Resource Limit exceeded&#34; to a technology stack diagram on the bottom of this page https://openwebsearch.eu/the-project/ being completely unreadable due to bad scaling.It is very disappointing really. Another example from the top of my head. Here in Poland we have ID cards(as every other EU country) . Those ID cards have to be renewed every now and then (10~15years). In last years an online system for government services was implemented  including for renewal of those cards. One could take a photo with a mobile phone, submit an application and pick up a card from a gov office in few weeks time. Unfortunately, EU made a law that ID cards applications have to be acommpanied by biometrics (fingerprints) so this system has been thrown away. One has to physically go to the gov office, scan their fingerprints, apply for a new id card and then go again to pick it up...Ok, so what happens in 10 years time? They should have the fingerprints already, right? No. They take the fingerprints, they store them only until one picks up the id card and then they are deleted. There are no fingerprint database, they are not stored anywhere. The fingerprints are used only to ensure the same person that submitted the application picks up the document. It makes zero sense, other than to break the previous online system. Thanks EU."</p>
							
						
						</a>
					</div>
				</div>
			</div>
		</div>
		
		<div class="story story_small-padding-bottom">
			
			<div >
				<div class="story__title">
					<h3><a href="https://storage.courtlistener.com/recap/gov.uscourts.nysd.524076/gov.uscourts.nysd.524076.247.0.pdf">Tether ordered to produce documents showing backing of USDT [pdf]</a></h3>
				</div>
				<div class="story__comments">
					<div class="story__comment">
						<a href="https://news.ycombinator.com/item?id=32926201">
						
							
								<p class="story__comment_first line-clamp">"(IANAL) Tether was trying to claim that requesting essentially a complete financial report for Tether and Bitfinex was overbroad, but has so far refused to produce any example of a more specific and limited set of information, so the court said &#34;open your books&#34;:  ...In the absence of agreement between the parties (id., Ex. 1 at 5
  (&#34;Plaintiffs remain open to considering an alternate proposal ... but
  cannot do without some indications of what the B/T Defendants intend to
  produce[.]&#34;)), the Court finds that Plaintiffs&#39; financial records RFPs
  are not overly broad, particularly given that Defendants have had
  opportunities to make sample productions of the financial records RFPs,
  but have failed to do so despite Plaintiffs&#39; agreement to such proposal
  (id., Ex. 1 at 5). Plaintiffs plainly explain why they need this
  information: to assess the backing of USDT with US dollars, and to
  allow a forensic accountant to assess the USDT reserve...
  
  The documents sought in the transactions RFPs appear to go to one of
  Plaintiffs&#39; core allegations: that the B/T Defendants engaged in
  cyptocommodities transactions using unbacked USDT, and that those
  transactions &#34;were strategically timed to inflate the market.&#34;...

  Accordingly, the Court ORDERS the B/T Defendants to produce documents in
  line with the revised RFPs 22-25, 29, 31, and 72"</p>
							
						
							
								<p class="story__comment_others">"With some financial engineering, an &#34;absolutely 100% dollar-backed&#34; asset can be backed by a &#34;more than 99% for-sure&#34; mix of assets, which are in turn backed by a &#34;96% good in even the worse case&#34; mish-mash, which are in turn backed by a &#34;I&#39;d bet my left arm that more than 91% are okay&#34; heap of stuff, which are in turn backed by a &#34;we have checked on 84% of &#39;em&#34; pile of smelly things, which are in turn..."</p>
							
						
							
								<p class="story__comment_others">"It&#39;s not backed. Once the Tether scam unravels, it&#39;s not clear how badly it will affect Bitcoin&#39;s price. But it&#39;s going to be bad.There&#39;s a chance that Tether has been pumping BTC."</p>
							
						
						</a>
					</div>
				</div>
			</div>
		</div>
		
		<div class="story story_small-padding-bottom">
			
			<div >
				<div class="story__title">
					<h3><a href="https://www.nvidia.com/en-us/geforce/graphics-cards/40-series/">GeForce RTX 40 Series</a></h3>
				</div>
				<div class="story__comments">
					<div class="story__comment">
						<a href="https://news.ycombinator.com/item?id=32912953">
						
							
								<p class="story__comment_first line-clamp">"These prices are beyond insulting and frankly I&#39;m glad they&#39;re going to take a hit competing with the flood of miner cards entering the market.Also note that nothing is preventing Optical Flow Acceleration [0] (and subsequently the DLSS 3.0 models that they claim are exclusive to the 40 series) from running on either 2/3 RTX cards. Just like RTX Voice and other gimmick &#34;exclusives&#34; I expect it to be available to older cards the moment they realize their backlog of 30 series cards aren&#39;t clearing as quickly as they thought.They&#39;re competing against an over-bloated secondhand market, AMD, Intel, much better integrated GPUs, and comparatively cheaper consoles that maintain sky-high demand with subsidized games via new subscription programs. They&#39;re vastly overestimating their brand loyalty (think Microsoft v Sony after the 360) and EVGA&#39;s exit makes more sense now than ever.[0] https://developer.nvidia.com/opticalflow-sdk"</p>
							
						
							
								<p class="story__comment_others">"So the 4090 is 38% faster (in FP32 FLOPS) than the current top server GPU (H100) and 105% faster than the current top desktop GPU (3090 Ti). And it&#39;s also more than twice as efficient (in FLOPS per watt) as all current top GPUs, even compared to the H100 which is manufactured on the same TSMC 4N process. This is impressive.The computing power of these new GPUs (in FP32 TFLOPS) is as below:  Nvidia RTX 4090:      82.6 TFLOPS (450 watts¹)
  Nvidia RTX 4080-16GB: 48.8 TFLOPS (320 watts¹)
  Nvidia RTX 4080-12GB: 40.1 TFLOPS (285 watts¹)

Compared to Nvidia&#39;s current top server and top desktop GPUs:  Nvidia H100 (SXM card): 60.1 TFLOPS (700 watts)
  Nvidia RTX 3090 Ti:     40.0 TFLOPS (450 watts)

Compared to AMD&#39;s current top server and top desktop GPUs:  AMD Instinct MI250X:    47.9 TFLOPS (560 watts)
  AMD Radeon RX 6950 XT:  23.7 TFLOPS (335 watts)

¹ I didn&#39;t see the wattage listed in this page by Nvidia; my source is https://www.digitaltrends.com/computing/nvidia-rtx-4090-rtx-..."</p>
							
						
							
								<p class="story__comment_others">"It seems like they&#39;re really emphasizing the difference in RT performance from the previous generation, but I think the gaming market at least will care more about the difference in raw frames and memory size from the previous generation and AMD&#39;s offerings.Personally, I like using RT for some single player showpiece games with a 3080 Ti, RT was useless on my 2080, but the games that I play the most do not use RT at all. DLSS is always great on any title that offers it, but again the real issue is that most of the games that people put real time into are the types of games for which RT is irrelevant. Graphical showpiece stuff is just a lot less relevant to the contemporary gaming market than it used to be."</p>
							
						
						</a>
					</div>
				</div>
			</div>
		</div>
		
		<div class="story story_small-padding-bottom">
			
			<div >
				<div class="story__title">
					<h3><a href="https://twitter.com/jonty/status/1571615998335123457">Unicode character “ꙮ” (U+A66E) is being updated</a></h3>
				</div>
				<div class="story__comments">
					<div class="story__comment">
						<a href="https://news.ycombinator.com/item?id=32896989">
						
							
								<p class="story__comment_first line-clamp">"When my kids were young, I accidentally flubbed the pronunciation of &#34;Santa Claus&#34; once and said something that sounded a lot like &#34;Centiclops&#34;, which I decided to roll with. Centiclops is a lot like a cyclops with one eye, except the as a reading of the roots clearly indicates, this is a creature with 100 eyes.Today I learn that Centiclops effectively has a Unicode character. As Centiclops&#39; representative in the world of the non-imaginary, we accept that a Unicode character with a hundred eyes is not practical and we accept the representation with just a few eyes, but generally agree that upgrading to 7 to 10 is a nice improvement, as 7 does not evenly divide into 100 but 10 does. This is important, because... reasons."</p>
							
						
							
								<p class="story__comment_others">"By the same reasoning, the 7-eyed O has now been used more than once, so it deserves a glyph! So the right way to do this is to introduce a new character for the correct glyph, and also leave the current one (perhaps changing the title). Otherwise these tweets won&#39;t make when read by someone that updated to Unicode 15.0"</p>
							
						
							
								<p class="story__comment_others">"Here[1][2] is the scan of manuscript from 1429, image #251[1] https://lib-fond.ru/lib-rgb/304-i/f-304i-308/#image-251
[2] https://web.archive.org/web/20110927102700/https://www.stsl...."</p>
							
						
						</a>
					</div>
				</div>
			</div>
		</div>
		
		<div class="story story_small-padding-bottom">
			
			<div >
				<div class="story__title">
					<h3><a href="https://freeman.vc/notes/aws-vs-gcp-reliability-is-wildly-different">AWS vs. GCP reliability is wildly different</a></h3>
				</div>
				<div class="story__comments">
					<div class="story__comment">
						<a href="https://news.ycombinator.com/item?id=32931096">
						
							
								<p class="story__comment_first line-clamp">"There were 84 errors for GCP, but the breakdown says 74 409s and 5 timeouts. Maybe it was 79 409s? Or 10 timeouts?I suspect the 409 conflicts are probably from the instance name not being unique in the test. It looks like the instance name used was:    instance_name = f&#34;gpu-test-{int(time())}&#34;

which has a 1-second precision. The test harness appears to do a `sleep(1)` between test creations, but this sort of thing can have weird boundary cases, particularly because (1) it does cleanup after creation, which will have variable latency, (2) `int()` will truncate the fractional part of the second from `time()`, and (3) `time.time()` is not monotonic.I would not ask the author to spend money to test it again, but I think the 409s would probably disappear if you replaced `int(time())` with `uuid.uuid4()`.Disclosure: I work at Google - on Google Compute Engine. :-)"</p>
							
						
							
								<p class="story__comment_others">"I wonder why someone would equate &#34;instance launch time&#34; with &#34;reliability&#34;... I won&#39;t go as far as calling it &#34;clickbait&#34; but wouldn&#39;t some other noun (&#34;startup performance is wildly different&#34;) have made more sense?"</p>
							
						
							
								<p class="story__comment_others">"Anecdotally I tend to agree with the author. But this really isn&#39;t a great way of comparing cloud services.The fundamental problem with cloud reliability is that it depends on a lot of stuff that&#39;s out of your control, that you have no visibility into. I have had services running happily on AWS with no errors, and the next month without changing anything they fail all the time.Why? Well, we look into it and it turns out AWS changed something behind the scenes. There&#39;s a different underlying hardware behind the instance, or some resource started being in high demand because of some other customers.So, I completely believe that at the time of this test, this particular API was performing a lot better on AWS than on GCP. But I wouldn&#39;t count on it still performing this way a month later. Cloud services aren&#39;t like a piece of dedicated hardware where you test it one month, and then the next month it behaves roughly the same. They are changing a lot of stuff that you can&#39;t see."</p>
							
						
						</a>
					</div>
				</div>
			</div>
		</div>
		

	

	<div class='about'>
		<a href='https://github.com/lopespm/hackernews-daily'>Fork me on GitHub</span>
	</div>

</div>

</body>
</html>