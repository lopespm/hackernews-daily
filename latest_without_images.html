<!doctype html>

<html lang="en">
<head>
	<title>Hacker News Daily</title>
	<meta name="description" content="Lightweight daily best Hacker News posts, with screenshots and top comments. No JavaScript used.">
	<meta name="author" content="Pedro Lopes">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta charset="utf-8">
	<link rel="stylesheet" href="css/styles.css?v=1.0">
	<link href="/favicon.png" rel="icon">
</head>

<body>

<div class="container">

	<div class="navbar">
		<div class="navbar__timespan">
			
				<span class="navbar__timespan_item-current navbar__timespan_item-spacing">Latest</span>
			
			
				<a href='all_without_images.html'>All</a>
			
		</div>
		<div class="navbar__no-images">
			
				<a href='index.html'
				>Enable Previews</a>
			
		</div>
	</div>

	
	<div class="day__date">01 September 2022</div>
		
		<div class="story story_small-padding-bottom">
			
			<div >
				<div class="story__title">
					<h3><a href="https://simonwillison.net/2022/Aug/29/stable-diffusion/">Stable Diffusion is a big deal</a></h3>
				</div>
				<div class="story__comments">
					<div class="story__comment">
						<a href="https://news.ycombinator.com/item?id=32634074">
						
							
								<p class="story__comment_first line-clamp">"After using SD heavily for a week, I half agree with this. It is incredibly disruptive, and it&#39;s wild how much it accelerates the creative process. I&#39;ll give you that.But two things I&#39;ve noticed:First, artists will still have a massive advantage over non-artists with this tool. A photographer who intimately knows the different lenses and cameras and industry terms will get to a representation of their idea much faster than someone without that experience. Without that depth of knowledge, someone might have to rely instead on random luck to create what&#39;s in their head. Art curators might be well-positioned here since having a wide breadth of knowledge and point of reference is their advantage.Second, we need the ability to persist a design. If I create a character using SD, I need to be able to persist that character across different scenarios, poses, emotions, lighting, etc. Based on what I know about the methods SD/Midjourney/Dall-E are using, I&#39;m not sure how easy this will be to implement, or if it&#39;s even possible at all. There will always be subtle differences and that&#39;s where being an artist who can use SD for inspiration instead of merely creation will retain their advantage over a non-artist.That said, holy crap. This tech is insane."</p>
							
						
							
								<p class="story__comment_others">"Is it just me, or do the comments in this thread seem to be the exact opposite of the sentiment in the comments on similar Github Copilot threads?I just find it a bit ironic that programmers are irate about Github Copilot using their copyrighted material to train. However, if it&#39;s an ML model training off of copyrighted artists material, clearly its a transformative work. I just find the opposing sentiments for these scenarios a bit funny."</p>
							
						
							
								<p class="story__comment_others">"&gt; Stable Diffusion has been trained on millions of copyrighted images scraped from the web.My brain has been trained on even more copyrighted material. Every book I read, every tv show I watch, the toys I played with as a child. It&#39;s hard to imagine that I could come up with anything that is not inspired by copyrighted work."</p>
							
						
						</a>
					</div>
				</div>
			</div>
		</div>
		
		<div class="story story_small-padding-bottom">
			
			<div >
				<div class="story__title">
					<h3><a href="https://replicate.com/andreasjansson/stable-diffusion-animation">Stable Diffusion animation</a></h3>
				</div>
				<div class="story__comments">
					<div class="story__comment">
						<a href="https://news.ycombinator.com/item?id=32658408">
						
							
								<p class="story__comment_first line-clamp">"Andreas, author of the Replicate model here -- though &#34;author&#34; feels wrong since I basically just stitched two amazing models together.The thing that really strikes me is that open source ML is starting to behave like open source software. I was able to take a pretrained text-to-image model and combine it with a pretrained video frame interpolation model and the two actually fit together! I didn&#39;t have to re-train or fine tune or map between incompatible embedding spaces, because these models can generalize to basically any image. I could treat these models as modular building blocks.It just makes your creative mind spin. What if you generate some speech with https://replicate.com/afiaka87/tortoise-tts, generate an image of an alien with Stable Diffusion, and then feed those two into https://replicate.com/wyhsirius/lia. Talking alien! Machine learning is starting to become really fun, even if you don&#39;t know anything about partial derivatives."</p>
							
						
							
								<p class="story__comment_others">"Maybe I can shine some light on the debate from an concept artist standpoint that works in VFX and advertising. I worked on feature films (3 of them in the imdb top 100), tv shows (like game of thrones) and hundreds of AD campaigns.In the last 10 year the work of a concept artist changed dramatically we have gone from purely painted concept art to mostly &#34;photobashed&#34;. Photobashed means basically that you rip apart other images and stitch them together to get the desired image. Some start with a rough sketch for the composition or make really rough grey shade 3d model and &#34;overpaint&#34; them. When it comes to &#34;photobashing&#34; the disregard for copyrights was always there and it&#39;s the worst in smaller studios and a bit better in the leading ones. Still most of the time everyone argues that if you only use really small parts of the images it is covered by fair use. There are some examples were studios got sued but mostly without bigger financial impact.A few months ago I started working with &#34;DiscoDiffusion&#34; to generate the images I use to photobash. &#34;DiscoDiffusion&#34; can produce great &#34;painterly&#34; images but struggles with photorealism and is slower, not as coherent as &#34;StableDiffusion&#34;.
Still the adoption rate in the concept art community was insanely fast. This all got topped by &#34;StableDiffusion&#34; in the last week. Ofc there are still people that want to do it the &#34;right&#34; way and not use AI but we had the same discussion years ago when &#34;photobashing&#34; came into place and some artists still wanted to paint the whole image. As concept artist you are mostly paid for your design thinking that means it is less about the process and more about the finished product. The turnaround time for styleframes got reduced from 3-4 hours while painting to 45 min - 1 hour when photobashing with stable diffusion me and my peers in the studio are now at 20-45 min per styleframe. When &#34;photobashing&#34; most people constrain themselves on their image library and ressources like Photobashing Kits. Not only does &#34;StableDiffusion&#34; cut the time in half it also gives greater freedom in composition and design especially if you are using  img2img.So where does this leave us? For the work in fast paced art environments like VFX, games, conept art or advertising &#34;StableDiffusion&#34; is a welcome gamechanger. Tradionalists and Artists outside of the industry might feel threatened but for us in these industries it&#39;s a god send."</p>
							
						
							
								<p class="story__comment_others">"I just came across this on twitter, every frame appears to be an evolution of the previous frame using img2img paired with a tilt/zoom to create a psychedelic animation.The author claims to have made this with Stable Diffusion, Disco, and Wiggle: https://www.youtube.com/watch?v=Nz_n0qxqoPgI believe Wiggle is used to automate the tilt/zoom between frames."</p>
							
						
						</a>
					</div>
				</div>
			</div>
		</div>
		
		<div class="story story_small-padding-bottom">
			
			<div >
				<div class="story__title">
					<h3><a href="https://dronexl.co/2022/08/22/dji-mavic-3-flies-over-mount-everest/">DJI Drone Flies over the Top of Mount Everest</a></h3>
				</div>
				<div class="story__comments">
					<div class="story__comment">
						<a href="https://news.ycombinator.com/item?id=32657112">
						
							
								<p class="story__comment_first line-clamp">"I&#39;ve seen hundreds of photos from the summit looking out, thousands of photos looking toward the summit from below, but this is the first time I saw a photo that really gives a feel for how much room there is at the summit. The point in the video shortly after he releases the drone at 1min 57sec feels like seeing the Earth from a spaceship or satellite for first time -- it&#39;s a perspective I&#39;d never seen before."</p>
							
						
							
								<p class="story__comment_others">"Side topic: DJI is the clear market leader, and it&#39;s also a Chinese company.Is there a viable alternative for EU customers that are sensible about the geopolitical aspects of this?I also looked at some open source firmware, but didn&#39;t find anything particularly appealing. e.g. [0][0]: https://www.parrot.com/en/open-source-drone-software"</p>
							
						
							
								<p class="story__comment_others">"What&#39;s really amazing is that they used a $2,000 drone, not something super super high end and specialized and out of reach. Plenty of hobby photographers have camera bodies, or combinations of camera body and lens, that cost that much.I have waded into the drone space recently. I had my eye on them (specifically DJI; they&#39;re clearly the leader) for a while (hobby photographer; lover of tech toys), but every time I looked in the past they were too expensive for me to justify. My uncle had one that as $1200 or so, and then a friend had one at like $600, and then just before we were going on vacation to Scotland I looked again and -- HOLY CRAP -- there&#39;s a model at $299.Sold.It&#39;s the DJI Mini SE, and you DO give up some stuff vs. the next level (about $500), but most of it is the drop from 4K video to &#34;merely&#34; HD. (There&#39;s also reduced range, which might end up mattering more.) OTOH $299 is for me a point where, if I fuck it up, I can be sanguine about it.It&#39;s INSANELY easy and fun to use. I got comfortable enough to send it far out over lochs, and to learn to trust its &#34;return to home&#34; function when flying in places where line of sight might obstruct the signal. If you haven&#39;t played with one you probably wonder how it handles losing a solid connection with the operator, and the answer is that, at least at this level, it remembers where it launched from, heads to a previously-set height (to avoid obstacles) and then comes home. At some point en route it&#39;ll re-acquire a connection and you can take back over.(Higher-end models (like the one they took to Everest) are able to navigate around obstacles and fly a more complex path home, but hey, this is the $299 model.)I&#39;m happy to share the video I shot but not publicly here. PM me (is that a thing?) if you want a link. And if you plan to travel somewhere beautiful where drone flight isn&#39;t severely restricted, I strongly suggest you consider getting one, China or not."</p>
							
						
						</a>
					</div>
				</div>
			</div>
		</div>
		
		<div class="story story_small-padding-bottom">
			
			<div >
				<div class="story__title">
					<h3><a href="https://krebsonsecurity.com/2022/08/final-thoughts-on-ubiquiti/">Final thoughts on Ubiquiti</a></h3>
				</div>
				<div class="story__comments">
					<div class="story__comment">
						<a href="https://news.ycombinator.com/item?id=32663296">
						
							
								<p class="story__comment_first line-clamp">"An accurate but pretty lacklustre &#34;mea culpa&#34; and retraction. I don&#39;t mind people making mistakes, everyone does, but seeing how Krebs has handled this whole episode has not inspired optimism in how he&#39;ll handle future mistakes.He was essentially used as an unwitting party in a cyber blackmail scheme, and he doesn&#39;t touch on that at all.  There will continue to be nefarious parties trying to misuse his reputation, so long as he remains a popular cyber researcher.  I wish he would show consciousness of that rather than simply saying &#34;I was wrong.&#34;"</p>
							
						
							
								<p class="story__comment_others">"&gt; As a result of the new information that has been provided to me, I no longer have faith in the veracity of my source or the information he provided to me. I always endeavor to ensure that my articles are properly sourced and factual.This is a strange statement given how the details of the FBI investigation have been public for a very long time.Krebs was fast to report on the initial accusations, but seems to have waited as long as possible to write about the revelations that his source was actually the perpetrator.&gt; This time, I missed the mark and, as a result, I would like to extend my sincerest apologies to Ubiquiti, and I have decided to remove those articles from my website.Given that Krebs is a reporter who has historically built a reputation on exposing things and bringing information to light, the brevity and vagueness of this article feels much more like a compromise to settle a lawsuit than typical reporting."</p>
							
						
							
								<p class="story__comment_others">"This seems to be the basics of the case:Initial report:
https://web.archive.org/web/20211202143043/https://krebsonse...Indictment of source:
https://web.archive.org/web/20211202161703/https://krebsonse...In cases like this it&#39;s probably better to leave the article up but plaster a big red &#39;retracted&#39; banner across it, with a link to a complete explanation as to why it was retracted.As far as defamation, isn&#39;t the legal bar on that pretty high in the USA?  Maybe there&#39;s a negligence issue, i.e. relying on a single source, not doing enough background, etc. that overrides the normal &#39;good faith&#39; reporting norms?"</p>
							
						
						</a>
					</div>
				</div>
			</div>
		</div>
		
		<div class="story story_small-padding-bottom">
			
			<div >
				<div class="story__title">
					<h3><a href="https://blog.tomayac.com/2022/08/30/things-not-available-when-someone-blocks-all-cookies/">Things not available when someone blocks all cookies</a></h3>
				</div>
				<div class="story__comments">
					<div class="story__comment">
						<a href="https://news.ycombinator.com/item?id=32661061">
						
							
								<p class="story__comment_first line-clamp">"I naively assumed from the headline that the author would complain about users blocking cookies. I was very pleasantly surprised to see a post written by someone who appreciates that some users will want to do this and is actively working to support delivering them a useful content experience!"</p>
							
						
							
								<p class="story__comment_others">"I often think that instead of completely blocking cookies, it would be better to accept them and then throw them away. Same with localStorage. Just store it temporarily."</p>
							
						
							
								<p class="story__comment_others">"What if browsers made it so when you turned off cookies, instead of not allowing anything to be written, they instead gave each page you visited its own fresh cookie jar that was cleared when you navigated away?"</p>
							
						
						</a>
					</div>
				</div>
			</div>
		</div>
		
		<div class="story story_small-padding-bottom">
			
			<div >
				<div class="story__title">
					<h3><a href="https://www.sec.gov/ix?doc=/Archives/edgar/data/1045810/000104581022000146/nvda-20220826.htm">US Government Bans Export of Nvidia A100 and H100 GPUs to China and Russia</a></h3>
				</div>
				<div class="story__comments">
					<div class="story__comment">
						<a href="https://news.ycombinator.com/item?id=32669215">
						
							
								<p class="story__comment_first line-clamp">"This doesn&#39;t just ban A100s. It bans those and &#34;any future NVIDIA integrated circuit achieving both peak performance and chip-to-chip I/O performance equal to or greater than thresholds that are roughly equivalent to the A100.&#34;It&#39;s a ceiling on the performance that can be exported. In an exponentially scaling industry, this is the equivalent of an announcement in advance of a complete ban on competitive products 2-3yrs out.Given that both political parties have placed export limits on tech to China, it seems this is the new normal."</p>
							
						
							
								<p class="story__comment_others">"I am not sure what exactly the thought process is here? So NVIDA is losing Chinese market, probably for the foreseeable future. Chinese are one step away from figuring out this technology anyway, if not already. They will not trust us again. It makes them less dependent on us. Oh, and we still need their manufacturing because of the labor costs, environmental issues, regulations and now, energy costs. How would this work exactly? I know it has been done during the Cold War I, but the world looks very different today."</p>
							
						
							
								<p class="story__comment_others">"There is a newly announced Chinese alternative to those GPUs - the Biren BR100.Fabbed at TSMC. (I guess we are about to see if SMIC can do their 7nm at scale.)https://www.nextplatform.com/2022/08/25/china-launches-the-i...CHINA LAUNCHES THE INEVITABLE INDIGENOUS GPU…Here is how Biren says it stacks up on various machine learning workloads, pitting the BR100 against the Nvidia A100:…We presume this is for AI training workloads, not inference. The average speedup over the A100 is around 2.6X. It is not clear if the Nvidia machines were using sparse matrix features, which doubles the throughput, or not. Our guess is they were not."</p>
							
						
						</a>
					</div>
				</div>
			</div>
		</div>
		
		<div class="story story_small-padding-bottom">
			
			<div >
				<div class="story__title">
					<h3><a href="https://waxy.org/2022/08/exploring-12-million-of-the-images-used-to-train-stable-diffusions-image-generator/">Exploring 12M of the 2.3B images used to train Stable Diffusion</a></h3>
				</div>
				<div class="story__comments">
					<div class="story__comment">
						<a href="https://news.ycombinator.com/item?id=32655497">
						
							
								<p class="story__comment_first line-clamp">"Hi, laion5b author here,Nice tool!You can also explore the dataset there https://rom1504.github.io/clip-retrieval/Thanks to approximate knn, it&#39;s possible to query and explore that 5B datasets with only 2TB of local storage, anyone can download the knn index and metadata to run that locally too.Regarding duplicates, indeed it&#39;s an interesting topic!Laion5b deduplicated samples by url+text, but not by image.To deduplicate by image you need to have an efficient way to compute whether image a and b are the same.An idea to do that is to compute an hash based on clip embeddings. A further idea would be to train a network actually good at dedup and not only similarity by training on positive and negative pairs, eg with triple loss.Here&#39;s my plan on the topic https://docs.google.com/document/d/1AryWpV0dD_r9x82I_quUzBuR...If anyone is interested to participate, I&#39;d be happy to guide them to do that. This is an open effort, just join laion discord server and let&#39;s talk."</p>
							
						
							
								<p class="story__comment_others">"Data is an excellent place to look at to get a sense of where the model is likely to work or not (what kinds of images), and for prompt design ideas because, roughly speaking, the probability of something working well is proportional to its frequency (or of things very similar to it) in the data.The story is more complex though because the data can often be quite far away from actual neural net training due to preprocessing steps, data augmentations, oversampling settings (it&#39;s not uncommon to not sample data uniformly during training), etc. So my favorite place to scrutinize is to build a &#34;batch explorer&#34;: During training of the network one dumps batches into pickles immediately before the forward pass of the neural net, then writes a separate explorer that loads the pickles and visualizes them to &#34;see exactly what the neural net sees&#34; during training. Ideally one then spends some quality time (~hours) looking through batches to get a qualitiative sense of what is likely to work or not work and how well. Of course this is also very useful for debugging, as many bugs can be present in the data preprocessing pipeline. But a batch explorer is harder to obtain here because you&#39;d need the full training data/code/settings."</p>
							
						
							
								<p class="story__comment_others">"If anyone is interested in the technical details, the database itself is a 4GB SQLite file which we are hosting with Datasette running on Fly.More details in our repo: https://github.com/simonw/laion-aesthetic-datasetteSearch is provided by SQLite FTS5."</p>
							
						
						</a>
					</div>
				</div>
			</div>
		</div>
		
		<div class="story story_small-padding-bottom">
			
			<div >
				<div class="story__title">
					<h3><a href="https://openai.com/blog/dall-e-introducing-outpainting/">DALL·E: Introducing Outpainting</a></h3>
				</div>
				<div class="story__comments">
					<div class="story__comment">
						<a href="https://news.ycombinator.com/item?id=32664507">
						
							
								<p class="story__comment_first line-clamp">"Meanwhile someone has already built a photoshop plugin for Stable Diffusion that you can use today to do basically the _exact_ same thing:https://old.reddit.com/r/StableDiffusion/comments/wyduk1/sho..."</p>
							
						
							
								<p class="story__comment_others">"So... are we done politely coughing and looking out the window at the idea that the gatekeeping was motivated by altruism so that we can move on and just use this much better innovation model going forward?Various (subjectively judged) SOTAs on at least some subset of at least this family of tasks is changing somewhere between daily and hourly right now. I&#39;ve been watching this stuff closely since fairly early ImageNet days and I&#39;ve never seen a Cambrian explosion of &#34;how the hell did that do that?&#34; events at anything like this cadence."</p>
							
						
							
								<p class="story__comment_others">"Feels like a race to the bottom. More features, lower cost, every week. No idea where it’ll level out, but I like it. Just bought some more Dalle credits today because it’s so much fun. This is a revolution in ‘art technology’ it’s like Steve Job’s bicycle for the mind. Best I could do a month ago was a stick figure in MS Paint, but now.."</p>
							
						
						</a>
					</div>
				</div>
			</div>
		</div>
		
		<div class="story story_small-padding-bottom">
			
			<div >
				<div class="story__title">
					<h3><a href="https://blog.cloudflare.com/cloudflares-abuse-policies-and-approach/">Cloudflare&#39;s abuse policies and approach</a></h3>
				</div>
				<div class="story__comments">
					<div class="story__comment">
						<a href="https://news.ycombinator.com/item?id=32661638">
						
							
								<p class="story__comment_first line-clamp">"It&#39;s rather disappointing that Cloudflare&#39;s policy is to not host content that is &#34;illegal, harmful, or violates the rights of others, including content that discloses sensitive personal information, incites or exploits violence against people or animals, or seeks to defraud the public&#34;, but they do not apply that same policy to content that they provide DDoS mitigation services for.I don&#39;t see why their policies should differ depending on whether they are hosting or protecting the content in question. Either way, they are in part responsible for making that content accessible. I get the feeling that this is just an arbitrary distinction that they&#39;ve made since hosting this content is more likely to have legal consequences for Cloudflare than simply providing DDoS mitigation services for it."</p>
							
						
							
								<p class="story__comment_others">"I find the article hopeful for the future of free speech on the internet. Even though the ability to host content on the net is one thing and lacking access to most platforms (where the users actually are) like youtube, facebook, instagram, tiktok is another. The most important thing nowadays is access to the siloed user bases for people who try to express dissident opinions, not the access to the net as whole, even though that is important as well.&gt;For instance, when a site that opposed LGBTQ+ rights signed up for a paid version of DDoS mitigation service we worked with our Proudflare employee resource group to identify an organization that supported LGBTQ+ rights and donate 100 percent of the fees for our services to them.I find this section peculiar. Why does the company have &#34;values&#34;? I though companies were supposed to be looking after the intrests of their shareholders, i.e., profit. Do the shareholders consent to the lost profit being donated to political motives? This broader trend of companies becoming political organizations is terrible, frankly.But even with that said, I find their stance in the article to be hopeful, and sincerely wish that they stay true to their words in this paragraph.&gt;To be clear, just because we did it in a limited set of cases before doesn’t mean we were right when we did. Or that we will ever do it again."</p>
							
						
							
								<p class="story__comment_others">"If this is in response to Kiwi Farms, I would say this is very disappointing.Love CloudFlare, think they are amazingly innovative, huge amount of respect for the people who work there.I see where they&#39;re coming from, but I don&#39;t see how KF is defensible whilst 8chan et al aren&#39;t."</p>
							
						
						</a>
					</div>
				</div>
			</div>
		</div>
		
		<div class="story story_small-padding-bottom">
			
			<div >
				<div class="story__title">
					<h3><a href="https://www.theverge.com/2022/8/31/23330123/snap-layoffs-announced-original-shows-canceled-games-mini-apps">Snap lays off 20% of employees</a></h3>
				</div>
				<div class="story__comments">
					<div class="story__comment">
						<a href="https://news.ycombinator.com/item?id=32662946">
						
							
								<p class="story__comment_first line-clamp">"Buried in the announcement is how Snap is shutting down Zenly and firing everyone who worked on it in Paris, France. Zenly is a location-based social application (think: you can see &amp; connect with your friends on a map). Snap bought them in 2017 for a reported $250-350M when they had ~4M installs. Now Zenly is at 40M MAUs, growing strong as I heard from employees working there.Zenly is (was?) popular in Japan, Southeast Asia, Eastern Europe and the Nordics. In Japan, it is head-on-head as the most used social app with Line.I find it odd that Snap just shuts down an app that was so popular in regions that is hard to gain foothold for any social media team. They could have likely sold it or spun it out, but chose not do so.It&#39;s also not like the app is a major cost to the company. Less than 100 people work on it across Snap, most based in Paris, France."</p>
							
						
							
								<p class="story__comment_others">"Evan also released a report this morning. Some departments such as Snap Minis &amp; games, snap originals, and a few other things like that pixy will be cut heavily/stopped entirely.Team members in the U.S. are stated to receive at least 4 months of &#34;compensation replacement&#34;. Those relying on work authorizations will receive additional support and flexibility according to the email.Other notable pieces is that Jerry Hunter is being promoted from SVP of engineering to COO.As a side note: I&#39;ve had several recruiters reach out to me since yesterday. Yesterday (08/30, day of verge report release) I had 9 unprompted recruiters reach out. Door dash, 2 from Amazon, and a few from some lesser known companies as well. This is with my LinkedIn profile set to &#34;not looking for a job&#34;.sources:[1] https://www.axios.com/2022/08/31/snap-restructuring-layoffs[2] I&#39;m a snap software engineer."</p>
							
						
							
								<p class="story__comment_others">"I&#39;ll never understand the reasoning for social media platforms to get into the hardware business.SNAP getting into drones and phones - you might as well just light stacks of money on fire, which is what happened here.Whatever happened to just doing what you are good at?  Or is the end result for every startup global world domination?"</p>
							
						
						</a>
					</div>
				</div>
			</div>
		</div>
		

	

	<div class='about'>
		<a href='https://github.com/lopespm/hackernews-daily'>Fork me on GitHub</span>
	</div>

</div>

</body>
</html>